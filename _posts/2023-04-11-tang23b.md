---
title: Wasserstein Distributional Learning via Majorization-Minimization
software: https://github.com/ChengliangTang/WDL_MM
abstract: Learning function-on-scalar predictive models for conditional densities
  and identifying factors that influence the entire probability distribution are vital
  tasks in many data-driven applications. We present an efficient Majorization-Minimization
  optimization algorithm, Wasserstein Distributional Learning (WDL), that trains Semi-parametric
  Conditional Gaussian Mixture Models (SCGMM) for conditional density functions and
  uses the Wasserstein distance $W_2$ as a proper metric for the space of density
  outcomes. We further provide theoretical convergence guarantees and illustrate the
  algorithm using boosted machines. Experiments on the synthetic data and real-world
  applications demonstrate the effectiveness of the proposed WDL algorithm.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tang23b
month: 0
tex_title: Wasserstein Distributional Learning via Majorization-Minimization
firstpage: 10703
lastpage: 10731
page: 10703-10731
order: 10703
cycles: false
bibtex_author: Tang, Chengliang and Lenssen, Nathan and Wei, Ying and Zheng, Tian
author:
- given: Chengliang
  family: Tang
- given: Nathan
  family: Lenssen
- given: Ying
  family: Wei
- given: Tian
  family: Zheng
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/tang23b/tang23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
