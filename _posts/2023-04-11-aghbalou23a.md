---
title: On the bias of K-fold cross validation with stable learners
abstract: This paper investigates the efficiency of the K-fold cross-validation (CV)
  procedure and a debiased version thereof as a means of estimating the generalization
  risk of a learning algorithm. We work under the general assumption of uniform algorithmic
  stability. We show that the K-fold risk estimate may not be consistent under such
  general stability assumptions, by constructing non vanishing lower bounds on the
  error in realistic contexts such as regularized empirical risk minimisation and
  stochastic gradient descent. We thus advocate the use of a debiased version of the
  K-fold and prove an error bound with exponential tail decay regarding this version.
  Our result is applicable to the large class of uniformly stable algorithms, contrarily
  to earlier works focusing on specific tasks such as density estimation. We illustrate
  the relevance of the debiased K-fold CV on a simple model selection problem and
  demonstrate empirically the usefulness of the promoted approach on real world classification
  and regression datasets.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: aghbalou23a
month: 0
tex_title: On the bias of K-fold cross validation with stable learners
firstpage: 3775
lastpage: 3794
page: 3775-3794
order: 3775
cycles: false
bibtex_author: Aghbalou, Anass and Sabourin, Anne and Portier, Fran\c{c}ois
author:
- given: Anass
  family: Aghbalou
- given: Anne
  family: Sabourin
- given: Fran√ßois
  family: Portier
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/aghbalou23a/aghbalou23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
