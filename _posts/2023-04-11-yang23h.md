---
title: Learning to Generalize Provably in Learning to Optimize
software: https://github.com/VITA-Group/Open-L2O/tree/main/Model_Free_L2O/L2O-Entropy
abstract: 'Learning to optimize (L2O) has gained increasing popularity, which automates
  the design of optimizers by data-driven approaches. However, current L2O methods
  often suffer from poor generalization performance in at least two folds: (i) applying
  the L2O-learned optimizer to unseen optimizees, in terms of lowering their loss
  function values (optimizer generalization, or “generalizable learning of optimizers”);
  and (ii) the test performance of an optimizee (itself as a machine learning model),
  trained by the optimizer, in terms of the accuracy over unseen data (optimizee generalization,
  or “learning to generalize”). While the optimizer generalization has been recently
  studied, the optimizee generalization (or learning to generalize) has not been rigorously
  studied in the L2O context, which is the aim of this paper. We first theoretically
  establish an implicit connection between the local entropy and the Hessian, and
  hence unify their roles in the handcrafted design of generalizable optimizers as
  equivalent metrics of the landscape flatness of loss functions. We then propose
  to incorporate these two metrics as flatness-aware regularizers into the L2O framework
  in order to meta-train optimizers to learn to generalize, and theoretically show
  that such generalization ability can be learned during the L2O meta-training process
  and then transformed to the optimizee loss function. Extensive experiments consistently
  validate the effectiveness of our proposals with substantially improved generalization
  on multiple sophisticated L2O models and diverse optimizees.'
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yang23h
month: 0
tex_title: Learning to Generalize Provably in Learning to Optimize
firstpage: 9807
lastpage: 9825
page: 9807-9825
order: 9807
cycles: false
bibtex_author: Yang, Junjie and Chen, Tianlong and Zhu, Mingkang and He, Fengxiang
  and Tao, Dacheng and Liang, Yingbin and Wang, Zhangyang
author:
- given: Junjie
  family: Yang
- given: Tianlong
  family: Chen
- given: Mingkang
  family: Zhu
- given: Fengxiang
  family: He
- given: Dacheng
  family: Tao
- given: Yingbin
  family: Liang
- given: Zhangyang
  family: Wang
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/yang23h/yang23h.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
