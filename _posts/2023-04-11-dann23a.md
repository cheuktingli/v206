---
title: Multiple-policy High-confidence Policy Evaluation
abstract: In reinforcement learning applications, we often want to accurately estimate
  the return of several policies of interest. We study this problem, multiple-policy
  high-confidence policy evaluation, where the goal is to estimate the return of all
  given target policies up to a desired accuracy with as few samples as possible.
  The natural approaches to this problem, i.e., evaluating each policy separately
  or estimating a model of the MDP, do not take into account the similarities between
  target policies and scale with the number of policies to evaluate or the size of
  the MDP, respectively. We present an alternative approach based on reusing samples
  from on-policy Monte-Carlo estimators and show that it is more sample-efficient
  in favorable cases. Specifically, we provide guarantees in terms of a notion of
  overlap of the set of target policies and shed light on when such an approach is
  indeed beneficial compared to existing methods.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dann23a
month: 0
tex_title: Multiple-policy High-confidence Policy Evaluation
firstpage: 9470
lastpage: 9487
page: 9470-9487
order: 9470
cycles: false
bibtex_author: Dann, Chris and Ghavamzadeh, Mohammad and Marinov, Teodor V.
author:
- given: Chris
  family: Dann
- given: Mohammad
  family: Ghavamzadeh
- given: Teodor V.
  family: Marinov
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/dann23a/dann23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
