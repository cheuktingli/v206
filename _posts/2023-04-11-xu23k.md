---
title: Benign overfitting of non-smooth neural networks beyond lazy training
abstract: Benign overfitting refers to a recently discovered intriguing phenomenon
  that over-parameterized neural networks, in many cases, can fit the training data
  perfectly but still generalize well, surprisingly contrary to the traditional belief
  that overfitting is harmful for generalization. In spite of its surging popularity
  in recent years, little has been known in the theoretical aspect of benign overfitting
  of neural networks. In this work, we provide a theoretical analysis of benign overfitting
  for two-layer neural networks with possibly non-smooth activation function. Without
  resorting to the popular Neural Tangent Kernel (NTK) approximation, we prove that
  neural networks can be trained with gradient descent to classify binary-labeled
  training data perfectly (achieving zero training loss) even in presence of polluted
  labels, but still generalize well. Our result removes the smoothness assumption
  in previous literature and goes beyond the NTK regime; this enables a better theoretical
  understanding of benign overfitting within a practically more meaningful setting,
  e.g., with (leaky-)ReLU activation function, small random initialization, and finite
  network width.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xu23k
month: 0
tex_title: Benign overfitting of non-smooth neural networks beyond lazy training
firstpage: 11094
lastpage: 11117
page: 11094-11117
order: 11094
cycles: false
bibtex_author: Xu, Xingyu and Gu, Yuantao
author:
- given: Xingyu
  family: Xu
- given: Yuantao
  family: Gu
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/xu23k/xu23k.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
