---
title: 'Federated Asymptotics: a model to compare federated learning algorithms'
software: https://github.com/garyxcheng/personalized-federated-learning
abstract: We develop an asymptotic framework to compare the test performance of (personalized)
  federated learning algorithms whose purpose is to move beyond algorithmic convergence
  arguments. To that end, we study a high-dimensional linear regression model to elucidate
  the statistical properties (per client test error) of loss minimizers. Our techniques
  and model allow precise predictions about the benefits of personalization and information
  sharing in federated scenarios, including that Federated Averaging with simple client
  fine-tuning achieves identical asymptotic risk to more intricate meta-learning approaches
  and outperforms naive Federated Averaging. We evaluate and corroborate these theoretical
  predictions on federated versions of the EMNIST, CIFAR-100, Shakespeare, and Stack
  Overflow datasets.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cheng23b
month: 0
tex_title: 'Federated Asymptotics: a model to compare federated learning algorithms'
firstpage: 10650
lastpage: 10689
page: 10650-10689
order: 10650
cycles: false
bibtex_author: Cheng, Gary and Chadha, Karan and Duchi, John
author:
- given: Gary
  family: Cheng
- given: Karan
  family: Chadha
- given: John
  family: Duchi
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/cheng23b/cheng23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
