---
title: 'Stochastic Gradient Descent-Ascent: Unified Theory and New Efficient Methods'
software: https://github.com/hugobb/sgda
abstract: Stochastic Gradient Descent-Ascent (SGDA) is one of the most prominent algorithms
  for solving min-max optimization and variational inequalities problems (VIP) appearing
  in various machine learning tasks. The success of the method led to several advanced
  extensions of the classical SGDA, including variants with arbitrary sampling, variance
  reduction, coordinate randomization, and distributed variants with compression,
  which were extensively studied in the literature, especially during the last few
  years. In this paper, we propose a unified convergence analysis that covers a large
  variety of stochastic gradient descent-ascent methods, which so far have required
  different intuitions, have different applications and have been developed separately
  in various communities. A key to our unified framework is a parametric assumption
  on the stochastic estimates. Via our general theoretical framework, we either recover
  the sharpest known rates for the known special cases or tighten them. Moreover,
  to illustrate the flexibility of our approach we develop several new variants of
  SGDA such as a new variance-reduced method (L-SVRGDA), new distributed methods with
  compression (QSGDA, DIANA-SGDA, VR-DIANA-SGDA), and a new method with coordinate
  randomization (SEGA-SGDA). Although variants of the new methods are known for solving
  minimization problems, they were never considered or analyzed for solving min-max
  problems and VIPs. We also demonstrate the most important properties of the new
  methods through extensive numerical experiments.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: beznosikov23a
month: 0
tex_title: 'Stochastic Gradient Descent-Ascent: Unified Theory and New Efficient Methods'
firstpage: 172
lastpage: 235
page: 172-235
order: 172
cycles: false
bibtex_author: Beznosikov, Aleksandr and Gorbunov, Eduard and Berard, Hugo and Loizou,
  Nicolas
author:
- given: Aleksandr
  family: Beznosikov
- given: Eduard
  family: Gorbunov
- given: Hugo
  family: Berard
- given: Nicolas
  family: Loizou
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/beznosikov23a/beznosikov23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
