---
title: Deep Neural Networks with Efficient Guaranteed Invariances
abstract: We address the problem of improving the performance and in particular the
  sample complexity of deep neural networks by enforcing and guaranteeing invariances
  to symmetry transformations rather than learning them from data. Group-equivariant
  convolutions are a popular approach to obtain equivariant representations. The desired
  corresponding invariance is then imposed using pooling operations. For rotations,
  it has been shown that using invariant integration instead of pooling further improves
  the sample complexity. In this contribution, we first expand invariant integration
  beyond rotations to flips and scale transformations. We then address the problem
  of incorporating multiple desired invariances into a single network. For this purpose,
  we propose a multi-stream architecture, where each stream is invariant to a different
  transformation such that the network can simultaneously benefit from multiple invariances.
  We demonstrate our approach with successful experiments on Scaled-MNIST, SVHN, CIFAR-10
  and STL-10.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: rath23a
month: 0
tex_title: Deep Neural Networks with Efficient Guaranteed Invariances
firstpage: 2460
lastpage: 2480
page: 2460-2480
order: 2460
cycles: false
bibtex_author: Rath, Matthias and Condurache, Alexandru Paul
author:
- given: Matthias
  family: Rath
- given: Alexandru Paul
  family: Condurache
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/rath23a/rath23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
