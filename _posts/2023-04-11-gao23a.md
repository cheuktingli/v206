---
title: On the Convergence of Distributed Stochastic Bilevel Optimization Algorithms
  over a Network
abstract: Bilevel optimization has been applied to a wide variety of machine learning
  models and numerous stochastic bilevel optimization algorithms have been developed
  in recent years. However, most existing algorithms restrict their focus on the single-machine
  setting so that they are incapable of handling the distributed data. To address
  this issue, under the setting where all participants compose a network and perform
  peer-to-peer communication in this network, we developed two novel decentralized
  stochastic bilevel optimization algorithms based on the gradient tracking communication
  mechanism and two different gradient estimators. Additionally, we established their
  convergence rates for nonconvex-strongly-convex problems with novel theoretical
  analysis strategies. To our knowledge, this is the first work achieving these theoretical
  results. Finally, we applied our algorithms to practical machine learning models,
  and the experimental results confirmed the efficacy of our algorithms.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gao23a
month: 0
tex_title: On the Convergence of Distributed Stochastic Bilevel Optimization Algorithms
  over a Network
firstpage: 9238
lastpage: 9281
page: 9238-9281
order: 9238
cycles: false
bibtex_author: Gao, Hongchang and Gu, Bin and Thai, My T.
author:
- given: Hongchang
  family: Gao
- given: Bin
  family: Gu
- given: My T.
  family: Thai
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/gao23a/gao23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
