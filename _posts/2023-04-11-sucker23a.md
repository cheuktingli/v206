---
title: PAC-Bayesian Learning of Optimization Algorithms
abstract: We apply the PAC-Bayes theory to the setting of learning-to-optimize. To
  the best of our knowledge, we present the first framework to learn optimization
  algorithms with provable generalization guarantees (PAC-bounds) and explicit trade-off
  between a high probability of convergence and a high convergence speed. Even in
  the limit case, where convergence is guaranteed, our learned optimization algorithms
  provably outperform related algorithms based on a (deterministic) worst-case analysis.
  Our results rely on PAC-Bayes bounds for general, unbounded loss-functions based
  on exponential families. By generalizing existing ideas, we reformulate the learning
  procedure into a one-dimensional minimization problem and study the possibility
  to find a global minimum, which enables the algorithmic realization of the learning
  procedure. As a proof-of-concept, we learn hyperparameters of standard optimization
  algorithms to empirically underline our theory.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sucker23a
month: 0
tex_title: PAC-Bayesian Learning of Optimization Algorithms
firstpage: 8145
lastpage: 8164
page: 8145-8164
order: 8145
cycles: false
bibtex_author: Sucker, Michael and Ochs, Peter
author:
- given: Michael
  family: Sucker
- given: Peter
  family: Ochs
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/sucker23a/sucker23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
