---
title: Explicit Regularization in Overparametrized Models via Noise Injection
software: https://github.com/aorvieto/noise_injection_overparam
abstract: Injecting noise within gradient descent has several desirable features,
  such as smoothing and regularizing properties. In this paper, we investigate the
  effects of injecting noise before computing a gradient step. We demonstrate that
  small perturbations can induce explicit regularization for simple models based on
  the L1-norm, group L1-norms, or nuclear norms. However, when applied to overparametrized
  neural networks with large widths, we show that the same perturbations can cause
  variance explosion. To overcome this, we propose using independent layer-wise perturbations,
  which provably allow for explicit regularization without variance explosion. Our
  empirical results show that these small perturbations lead to improved generalization
  performance compared to vanilla gradient descent.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: orvieto23a
month: 0
tex_title: Explicit Regularization in Overparametrized Models via Noise Injection
firstpage: 7265
lastpage: 7287
page: 7265-7287
order: 7265
cycles: false
bibtex_author: Orvieto, Antonio and Raj, Anant and Kersting, Hans and Bach, Francis
author:
- given: Antonio
  family: Orvieto
- given: Anant
  family: Raj
- given: Hans
  family: Kersting
- given: Francis
  family: Bach
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/orvieto23a/orvieto23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
