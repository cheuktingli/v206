---
title: Implicit Graphon Neural Representation
software: https://github.com/Mishne-Lab/IGNR
abstract: Graphons are general and powerful models for generating graphs of varying
  size. In this paper, we propose to directly model graphons using neural networks,
  obtaining Implicit Graphon Neural Representation (IGNR). Existing work in modeling
  and reconstructing graphons often approximates a target graphon by a fixed resolution
  piece-wise constant representation. Our IGNR has the benefit that it can represent
  graphons up to arbitrary resolutions, and enables natural and efficient generation
  of arbitrary sized graphs with desired structure once the model is learned. Furthermore,
  we allow the input graph data to be unaligned and have different sizes by leveraging
  the Gromov-Wasserstein distance. We first demonstrate the effectiveness of our model
  by showing its superior performance on a graphon learning task. We then propose
  an extension of IGNR that can be incorporated into an auto-encoder framework, and
  demonstrate its good performance under a more general setting of graphon learning.
  We also show that our model is suitable for graph representation learning and graph
  generation.
section: Notable Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xia23b
month: 0
tex_title: Implicit Graphon Neural Representation
firstpage: 10619
lastpage: 10634
page: 10619-10634
order: 10619
cycles: false
bibtex_author: Xia, Xinyue and Mishne, Gal and Wang, Yusu
author:
- given: Xinyue
  family: Xia
- given: Gal
  family: Mishne
- given: Yusu
  family: Wang
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/xia23b/xia23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
