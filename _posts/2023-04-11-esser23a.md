---
title: Improved Representation Learning Through Tensorized Autoencoders
software: https://github.com/mahalakshmi-sabanayagam/tensorized_autoencoder
abstract: The central question in representation learning is what constitutes a good
  or meaningful representation. In this work we argue that if we consider data with
  inherent cluster structures, where clusters can be characterized through different
  means and covariances, those data structures should be represented in the embedding
  as well. While Autoencoders (AE) are widely used in practice for unsupervised representation
  learning, they do not fulfil the above condition on the embedding as they obtain
  a single representation of the data. To overcome this we propose a meta-algorithm
  that can be used to extend an arbitrary AE architecture to a tensorized version
  (TAE) that allows for learning cluster-specific embeddings while simultaneously
  learning the cluster assignment. For the linear setting we prove that TAE can recover
  the principle components of the different clusters in contrast to principle component
  of the entire data recovered by a standard AE. We validate this on planted models
  and for general, non-linear and convolutional AEs we empirically illustrate that
  tensorizing the AE is beneficial in clustering and de-noising tasks.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: esser23a
month: 0
tex_title: Improved Representation Learning Through Tensorized Autoencoders
firstpage: 8294
lastpage: 8307
page: 8294-8307
order: 8294
cycles: false
bibtex_author: Esser, Pascal and Mukherjee, Satyaki and Sabanayagam, Mahalakshmi and
  Ghoshdastidar, Debarghya
author:
- given: Pascal
  family: Esser
- given: Satyaki
  family: Mukherjee
- given: Mahalakshmi
  family: Sabanayagam
- given: Debarghya
  family: Ghoshdastidar
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/esser23a/esser23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
