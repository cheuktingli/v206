---
title: A Bregman Divergence View on the Difference-of-Convex Algorithm
abstract: The difference of convex (DC) algorithm is a conceptually simple method
  for the minimization of (non)convex functions that are expressed as the difference
  of two convex functions. An attractive feature of the algorithm is that it maintains
  a global overestimator on the function and does not require a choice of step size
  at each iteration. By adopting a Bregman divergence point of view, we simplify and
  strengthen many existing non-asymptotic convergence guarantees for the DC algorithm.
  We further present several sufficient conditions that ensure a linear convergence
  rate, namely a new DC Polyak-Lojasiewicz condition, as well as a relative strong
  convexity assumption. Importantly, our conditions do not require smoothness of the
  objective function. We illustrate our results on a family of minimization problems
  involving the quantum relative entropy, with applications in quantum information
  theory.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: faust23a
month: 0
tex_title: A Bregman Divergence View on the Difference-of-Convex Algorithm
firstpage: 3427
lastpage: 3439
page: 3427-3439
order: 3427
cycles: false
bibtex_author: Faust, Oisin and Fawzi, Hamza and Saunderson, James
author:
- given: Oisin
  family: Faust
- given: Hamza
  family: Fawzi
- given: James
  family: Saunderson
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/faust23a/faust23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
