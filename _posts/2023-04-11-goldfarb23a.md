---
title: Analysis of Catastrophic Forgetting for Random Orthogonal Transformation Tasks
  in the Overparameterized Regime
abstract: Overparameterization is known to permit strong generalization performance
  in neural networks. In this work, we provide an initial theoretical analysis of
  its effect on catastrophic forgetting in a continual learning setup. We show experimentally
  that in Permuted MNIST image classification tasks, the generalization performance
  of multilayer perceptrons trained by vanilla stochastic gradient descent can be
  improved by overparameterization, and the extent of the performance increase achieved
  by overparameterization is comparable to that of state-of-the-art continual learning
  algorithms. We provide a theoretical explanation of this effect by studying a qualitatively
  similar two-task linear regression problem, where each task is related by a random
  orthogonal transformation. We show that when a model is trained on the two tasks
  in sequence without any additional regularization, the risk gain on the first task
  is small if the model is sufficiently overparameterized.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: goldfarb23a
month: 0
tex_title: Analysis of Catastrophic Forgetting for Random Orthogonal Transformation
  Tasks in the Overparameterized Regime
firstpage: 2975
lastpage: 2993
page: 2975-2993
order: 2975
cycles: false
bibtex_author: Goldfarb, Daniel and Hand, Paul
author:
- given: Daniel
  family: Goldfarb
- given: Paul
  family: Hand
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/goldfarb23a/goldfarb23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
