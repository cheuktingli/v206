---
title: 'Federated Averaging Langevin Dynamics: Toward a unified theory and new algorithms'
abstract: This paper focuses on Bayesian inference in a federated learning context
  (FL). While several distributed MCMC algorithms have been proposed, few consider
  the specific limitations of FL such as communication bottlenecks and statistical
  heterogeneity. Recently, Federated Averaging Langevin Dynamics (FALD) was introduced,
  which extends the Federated Averaging algorithm to Bayesian inference. We obtain
  a novel tight non-asymptotic upper bound on the Wasserstein distance to the global
  posterior for FALD. This bound highlights the effects of statistical heterogeneity,
  which causes a drift in the local updates that negatively impacts convergence. We
  propose a new algorithm VR-FALD* that uses control variates to correct the client
  drift. We establish non-asymptotic bounds showing that VR-FALD* is not affected
  by statistical heterogeneity. Finally, we illustrate our results on several FL benchmarks
  for Bayesian inference.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: plassier23a
month: 0
tex_title: 'Federated Averaging Langevin Dynamics: Toward a unified theory and new
  algorithms'
firstpage: 5299
lastpage: 5356
page: 5299-5356
order: 5299
cycles: false
bibtex_author: Plassier, Vincent and Moulines, Eric and Durmus, Alain
author:
- given: Vincent
  family: Plassier
- given: Eric
  family: Moulines
- given: Alain
  family: Durmus
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/plassier23a/plassier23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
