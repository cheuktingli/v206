---
title: Variational Inference for Neyman-Scott Processes
software: https://github.com/hongchengkuan/Inclusive_VI_NSPs
abstract: Neyman-Scott processes (NSPs) have been applied across a range of fields
  to model points or temporal events with a hierarchy of clusters. Markov chain Monte
  Carlo (MCMC) is typically used for posterior sampling in the model. However, MCMCâ€™s
  mixing time can cause the resulting inference to be slow, and thereby slow down
  model learning and prediction. We develop the first variational inference (VI) algorithm
  for NSPs, and give two examples of suitable variational posterior point process
  distributions. Our method minimizes the inclusive Kullback-Leibler (KL) divergence
  for VI to obtain the variational parameters. We generate samples from the approximate
  posterior point processes much faster than MCMC, as we can directly estimate the
  approximate posterior point processes without any MCMC steps or gradient descent.
  We include synthetic and real-world data experiments that demonstrate our VI algorithm
  achieves better prediction performance than MCMC when computational time is limited.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hong23a
month: 0
tex_title: Variational Inference for Neyman-Scott Processes
firstpage: 2002
lastpage: 2018
page: 2002-2018
order: 2002
cycles: false
bibtex_author: Hong, Chengkuan and Shelton, Christian
author:
- given: Chengkuan
  family: Hong
- given: Christian
  family: Shelton
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/hong23a/hong23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
