---
title: Sequential Gradient Descent and Quasi-Newton’s Method for Change-Point Analysis
abstract: One common approach to detecting change-points is minimizing a cost function
  over possible numbers and locations of change-points. The framework includes several
  well-established procedures, such as the penalized likelihood and minimum description
  length. Such an approach requires finding the cost value repeatedly over different
  segments of the data set, which can be time-consuming when (i) the data sequence
  is long and (ii) obtaining the cost value involves solving a non-trivial optimization
  problem. This paper introduces a new sequential updating method (SE) to find the
  cost value effectively. The core idea is to update the cost value using the information
  from previous steps without re-optimizing the objective function. The new method
  is applied to change-point detection in generalized linear models and penalized
  regression. Numerical studies show that the new approach can be orders of magnitude
  faster than the Pruned Exact Linear Time (PELT) method without sacrificing estimation
  accuracy.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang23b
month: 0
tex_title: Sequential Gradient Descent and Quasi-Newton’s Method for Change-Point
  Analysis
firstpage: 1129
lastpage: 1143
page: 1129-1143
order: 1129
cycles: false
bibtex_author: Zhang, Xianyang and Dawn, Trisha
author:
- given: Xianyang
  family: Zhang
- given: Trisha
  family: Dawn
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/zhang23b/zhang23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
