---
title: Global Convergence of Over-parameterized Deep Equilibrium Models
software: https://github.com/snowbbbb/code-for-Global-Convergence-of-Over-parameterized-Deep-Equilibrium-Models
abstract: A deep equilibrium model (DEQ) is implicitly defined through an equilibrium
  point of an infinite-depth weight-tied model with an input-injection. Instead of
  infinite computations, it solves an equilibrium point directly with root-finding
  and computes gradients with implicit differentiation. In this paper, the training
  dynamics of over-parameterized DEQs are investigated, and we propose a novel probabilistic
  framework to overcome the challenge arising from the weight-sharing and the infinite
  depth. By supposing a condition on the initial equilibrium point, we prove that
  the gradient descent converges to a globally optimal solution at a linear convergence
  rate for the quadratic loss function. We further perform a fine-grained non-asymptotic
  analysis about random DEQs and the corresponding weight-untied models, and show
  that the required initial condition is satisfied via mild over-parameterization.
  Moreover, we show that the unique equilibrium point always exists during the training.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ling23a
month: 0
tex_title: Global Convergence of Over-parameterized Deep Equilibrium Models
firstpage: 767
lastpage: 787
page: 767-787
order: 767
cycles: false
bibtex_author: Ling, Zenan and Xie, Xingyu and Wang, Qiuhao and Zhang, Zongpeng and
  Lin, Zhouchen
author:
- given: Zenan
  family: Ling
- given: Xingyu
  family: Xie
- given: Qiuhao
  family: Wang
- given: Zongpeng
  family: Zhang
- given: Zhouchen
  family: Lin
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/ling23a/ling23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
