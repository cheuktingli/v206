---
title: 'TabLLM: Few-shot Classification of Tabular Data with Large Language Models'
software: https://github.com/clinicalml/TabLLM
abstract: We study the application of large language models to zero-shot and few-shot
  classification of tabular data. We prompt the large language model with a serialization
  of the tabular data to a natural-language string, together with a short description
  of the classification problem. In the few-shot setting, we fine-tune the large language
  model using some labeled examples. We evaluate several serialization methods including
  templates, table-to-text models, and large language models. Despite its simplicity,
  we find that this technique outperforms prior deep-learning-based tabular classification
  methods on several benchmark datasets. In most cases, even zero-shot classification
  obtains non-trivial performance, illustrating the methodâ€™s ability to exploit prior
  knowledge encoded in large language models. Unlike many deep learning methods for
  tabular datasets, this approach is also competitive with strong traditional baselines
  like gradient-boosted trees, especially in the very-few-shot setting.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hegselmann23a
month: 0
tex_title: 'TabLLM: Few-shot Classification of Tabular Data with Large Language Models'
firstpage: 5549
lastpage: 5581
page: 5549-5581
order: 5549
cycles: false
bibtex_author: Hegselmann, Stefan and Buendia, Alejandro and Lang, Hunter and Agrawal,
  Monica and Jiang, Xiaoyi and Sontag, David
author:
- given: Stefan
  family: Hegselmann
- given: Alejandro
  family: Buendia
- given: Hunter
  family: Lang
- given: Monica
  family: Agrawal
- given: Xiaoyi
  family: Jiang
- given: David
  family: Sontag
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/hegselmann23a/hegselmann23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
