---
title: Efficiently Forgetting What You Have Learned in Graph Representation Learning
  via Projection
abstract: As privacy protection receives much attention, unlearning the effect of
  a specific node from a pre-trained graph learning model has become equally important.
  However, due to the node dependency in the graph-structured data, representation
  unlearning in Graph Neural Networks (GNNs) is challenging and less well explored.
  In this paper, we fill in this gap by first studying the unlearning problem in linear-GNNs,
  and then introducing its extension to non-linear structures. Given a set of nodes
  to unlearn, we propose Projector that unlearns by projecting the weight parameters
  of the pre-trained model onto a subspace that is irrelevant to features of the nodes
  to be forgotten. Projector could overcome the challenges caused by node dependency
  and enjoys perfect data removal, i.e., the unlearned model parameters do not contain
  any information about the unlearned node features which is guaranteed by algorithmic
  construction. Empirical results on real-world datasets illustrate the effectiveness
  and efficiency of Projector.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cong23a
month: 0
tex_title: Efficiently Forgetting What You Have Learned in Graph Representation Learning
  via Projection
firstpage: 6674
lastpage: 6703
page: 6674-6703
order: 6674
cycles: false
bibtex_author: Cong, Weilin and Mahdavi, Mehrdad
author:
- given: Weilin
  family: Cong
- given: Mehrdad
  family: Mahdavi
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/cong23a/cong23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
