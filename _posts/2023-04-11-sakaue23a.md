---
title: Improved Generalization Bound and Learning of Sparsity Patterns for Data-Driven
  Low-Rank Approximation
abstract: Learning sketching matrices for fast and accurate low-rank approximation
  (LRA) has gained increasing attention. Recently, Bartlett, Indyk, and Wagner (COLT
  2022) presented a generalization bound for the learning-based LRA. Specifically,
  for rank-$k$ approximation using an $m \times n$ learned sketching matrix with $s$
  non-zeros in each column, they proved an $\tilde O(nsm)$ bound on the fat shattering
  dimension ($\tilde O$ hides logarithmic factors). We build on their work and make
  two contributions. (1) We present a better $\tilde O(nsk)$ bound ($k \le m$). En
  route to obtaining this result, we give a low-complexity Goldbergâ€“Jerrum algorithm
  for computing pseudo-inverse matrices, which would be of independent interest. (2)
  We alleviate an assumption of the previous study that sketching matrices have a
  fixed sparsity pattern. We prove that learning positions of non-zeros increases
  the fat shattering dimension only by $O(ns\log n)$. In addition, experiments confirm
  the practical benefit of learning sparsity patterns.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sakaue23a
month: 0
tex_title: Improved Generalization Bound and Learning of Sparsity Patterns for Data-Driven
  Low-Rank Approximation
firstpage: 1
lastpage: 10
page: 1-10
order: 1
cycles: false
bibtex_author: Sakaue, Shinsaku and Oki, Taihei
author:
- given: Shinsaku
  family: Sakaue
- given: Taihei
  family: Oki
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/sakaue23a/sakaue23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
