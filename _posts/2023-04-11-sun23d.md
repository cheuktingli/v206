---
title: Convergence of Stein Variational Gradient Descent under a Weaker Smoothness
  Condition
software: https://github.com/Iwillnottellyou/l0l1-SVGD.git.
abstract: Stein Variational Gradient Descent (SVGD) is an important alternative to
  the Langevin-type algorithms for sampling from probability distributions of the
  form $\pi(x) \propto \exp(-V(x))$. In the existing theory of Langevin-type algorithms
  and SVGD, the potential function $V$ is often assumed to be $L$-smooth. However,
  this restrictive condition excludes a large class of potential functions such as
  polynomials of degree greater than $2$. Our paper studies the convergence of the
  SVGD algorithm for distributions with $(L_0,L_1)$-smooth potentials. This relaxed
  smoothness assumption was introduced by Zhang et al. [2019a] for the analysis of
  gradient clipping algorithms. With the help of trajectory-independent auxiliary
  conditions, we provide a descent lemma establishing that the algorithm decreases
  the KL divergence at each iteration and prove a complexity bound for SVGD in the
  population limit in terms of the Stein Fisher information.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sun23d
month: 0
tex_title: Convergence of Stein Variational Gradient Descent under a Weaker Smoothness
  Condition
firstpage: 3693
lastpage: 3717
page: 3693-3717
order: 3693
cycles: false
bibtex_author: Sun, Lukang and Karagulyan, Avetik and Richtarik, Peter
author:
- given: Lukang
  family: Sun
- given: Avetik
  family: Karagulyan
- given: Peter
  family: Richtarik
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/sun23d/sun23d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
