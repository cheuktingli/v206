---
title: A Case of Exponential Convergence Rates for SVM
abstract: Optimizing the misclassification risk is in general NP-hard. Tractable solvers
  can be obtained by considering a surrogate regression problem. While convergence
  to the regression function is typically sublinear, the corresponding classification
  error can decay much faster. Fast and super fast rates (up to exponential) have
  been established for general smooth losses on problems where a hard margin is present
  between classes. This leaves out models based on non-smooth losses such as support
  vector machines, and problems where there is no hard margin, begging several questions.
  Are such models incapable of fast convergence? Are they therefore structurally inferior?
  Is the hard margin condition really necessary to obtain exponential convergence?
  Developing a new strategy, we provide an answer to these questions. In particular,
  we show not only that support vector machines can indeed converge exponentially
  fast, but also that they can do so even without hard margin.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cabannnes23a
month: 0
tex_title: A Case of Exponential Convergence Rates for SVM
firstpage: 359
lastpage: 374
page: 359-374
order: 359
cycles: false
bibtex_author: Cabannnes, Vivien and Vigogna, Stefano
author:
- given: Vivien
  family: Cabannnes
- given: Stefano
  family: Vigogna
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/cabannnes23a/cabannnes23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
