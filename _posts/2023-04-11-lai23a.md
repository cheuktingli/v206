---
title: Robust Variational Autoencoding with Wasserstein Penalty for Novelty Detection
software: https://github.com/JCL823/MAW
abstract: 'We propose a new method for novelty detection that can tolerate high corruption
  of the training points, whereas previous works assumed either no or very low corruption.
  Our method trains a robust variational autoencoder (VAE), which aims to generate
  a model for the uncorrupted training points. To gain robustness to high corruption,
  we incorporate the following four changes to the common VAE: 1. Extracting crucial
  features of the latent code by a carefully designed dimension reduction component
  for distributions; 2. Modeling the latent distribution as a mixture of Gaussian
  low-rank inliers and full-rank outliers, where the testing only uses the inlier
  model; 3. Applying the Wasserstein-1 metric for regularization, instead of the Kullback-Leibler
  (KL) divergence; and 4. Using a robust error for reconstruction. We establish both
  robustness to outliers and suitability to low-rank modeling of the Wasserstein metric
  as opposed to the KL divergence. We illustrate state-of-the-art results on standard
  benchmarks.'
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lai23a
month: 0
tex_title: Robust Variational Autoencoding with Wasserstein Penalty for Novelty Detection
firstpage: 3538
lastpage: 3567
page: 3538-3567
order: 3538
cycles: false
bibtex_author: Lai, Chieh-Hsin and Zou, Dongmian and Lerman, Gilad
author:
- given: Chieh-Hsin
  family: Lai
- given: Dongmian
  family: Zou
- given: Gilad
  family: Lerman
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/lai23a/lai23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
