---
title: Convex Bounds on the Softmax Function with Applications to Robustness Verification
software: https://github.com/NeuralNetworkVerification/bounding-softmax/tree/aistats
abstract: The softmax function is a ubiquitous component at the output of neural networks
  and increasingly in intermediate layers as well. This paper provides convex lower
  bounds and concave upper bounds on the softmax function, which are compatible with
  convex optimization formulations for characterizing neural networks and other ML
  models. We derive bounds using both a natural exponential-reciprocal decomposition
  of the softmax as well as an alternative decomposition in terms of the log-sum-exp
  function. The new bounds are provably and/or numerically tighter than linear bounds
  obtained in previous work on robustness verification of transformers. As illustrations
  of the utility of the bounds, we apply them to verification of transformers as well
  as of the robustness of predictive uncertainty estimates of deep ensembles.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wei23c
month: 0
tex_title: Convex Bounds on the Softmax Function with Applications to Robustness Verification
firstpage: 6853
lastpage: 6878
page: 6853-6878
order: 6853
cycles: false
bibtex_author: Wei, Dennis and Wu, Haoze and Wu, Min and Chen, Pin-Yu and Barrett,
  Clark and Farchi, Eitan
author:
- given: Dennis
  family: Wei
- given: Haoze
  family: Wu
- given: Min
  family: Wu
- given: Pin-Yu
  family: Chen
- given: Clark
  family: Barrett
- given: Eitan
  family: Farchi
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/wei23c/wei23c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
