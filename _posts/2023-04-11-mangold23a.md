---
title: High-Dimensional Private Empirical Risk Minimization by Greedy Coordinate Descent
software: https://gitlab.inria.fr/pmangold1/greedy-coordinate-descent
abstract: In this paper, we study differentially private empirical risk minimization
  (DP-ERM). It has been shown that the worst-case utility of DP-ERM reduces polynomially
  as the dimension increases. This is a major obstacle to privately learning large
  machine learning models. In high dimension, it is common for some model’s parameters
  to carry more information than others. To exploit this, we propose a differentially
  private greedy coordinate descent (DP-GCD) algorithm. At each iteration, DP-GCD
  privately performs a coordinate-wise gradient step along the gradients’ (approximately)
  greatest entry. We show theoretically that DP-GCD can achieve a logarithmic dependence
  on the dimension for a wide range of problems by naturally exploiting their structural
  properties (such as quasi-sparse solutions). We illustrate this behavior numerically,
  both on synthetic and real datasets.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: mangold23a
month: 0
tex_title: High-Dimensional Private Empirical Risk Minimization by Greedy Coordinate
  Descent
firstpage: 4894
lastpage: 4916
page: 4894-4916
order: 4894
cycles: false
bibtex_author: Mangold, Paul and Bellet, Aur\'elien and Salmon, Joseph and Tommasi,
  Marc
author:
- given: Paul
  family: Mangold
- given: Aurélien
  family: Bellet
- given: Joseph
  family: Salmon
- given: Marc
  family: Tommasi
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/mangold23a/mangold23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
