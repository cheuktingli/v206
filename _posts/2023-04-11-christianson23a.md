---
title: Optimal robustness-consistency tradeoffs for learning-augmented metrical task
  systems
abstract: We examine the problem of designing learning-augmented algorithms for metrical
  task systems (MTS) that exploit machine-learned advice while maintaining rigorous,
  worst-case guarantees on performance. We propose an algorithm, DART, that achieves
  this dual objective, providing cost within a multiplicative factor $(1+\epsilon)$
  of the machine-learned advice (i.e., consistency) while ensuring cost within a multiplicative
  factor $2^{O(1/\epsilon)}$ of a baseline robust algorithm (i.e., robustness) for
  any $\epsilon > 0$. We show that this exponential tradeoff between consistency and
  robustness is unavoidable in general, but that in important subclasses of MTS, such
  as when the metric space has bounded diameter and in the $k$-server problem, our
  algorithm achieves improved, polynomial tradeoffs between consistency and robustness.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: christianson23a
month: 0
tex_title: Optimal robustness-consistency tradeoffs for learning-augmented metrical
  task systems
firstpage: 9377
lastpage: 9399
page: 9377-9399
order: 9377
cycles: false
bibtex_author: Christianson, Nicolas and Shen, Junxuan and Wierman, Adam
author:
- given: Nicolas
  family: Christianson
- given: Junxuan
  family: Shen
- given: Adam
  family: Wierman
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/christianson23a/christianson23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
