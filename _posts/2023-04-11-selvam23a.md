---
title: Mixtures of All Trees
software: https://github.com/UCLA-StarAI/MoAT
abstract: 'Tree-shaped graphical models are widely used for their tractability. However,
  they unfortunately lack expressive power as they require committing to a particular
  sparse dependency structure. We propose a novel class of generative models called
  mixtures of all trees: that is, a mixture over all possible ($n^{n-2}$) tree-shaped
  graphical models over n variables. We show that it is possible to parameterize this
  Mixture of All Trees (MoAT) model compactly (using a polynomial-size representation)
  in a way that allows for tractable likelihood computation and optimization via stochastic
  gradient descent. Furthermore, by leveraging the tractability of tree-shaped models,
  we devise fast-converging conditional sampling algorithms for approximate inference,
  even though our theoretical analysis suggests that exact computation of marginals
  in the MoAT model is NP-hard. Empirically, MoAT achieves state-of-the-art performance
  on density estimation benchmarks when compared against powerful probabilistic models
  including hidden Chow-Liu Trees.'
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: selvam23a
month: 0
tex_title: Mixtures of All Trees
firstpage: 11043
lastpage: 11058
page: 11043-11058
order: 11043
cycles: false
bibtex_author: Selvam, Nikil Roashan and Zhang, Honghua and Van den Broeck, Guy
author:
- given: Nikil Roashan
  family: Selvam
- given: Honghua
  family: Zhang
- given: Guy
  family: Broeck
  prefix: Van den
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/selvam23a/selvam23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
