---
title: Competing against Adaptive Strategies in Online Learning via Hints
abstract: For many of the classic online learning settings, it is known that having
  a “hint” about the loss function before making a prediction yields significantly
  better regret guarantees. In this work we study the question, do hints allow us
  to go beyond the standard notion of regret (which competes against the best fixed
  strategy) and compete against adaptive or dynamic strategies? After all, if hints
  were perfect, we can clearly compete against a fully dynamic strategy. For some
  common online learning settings, we provide upper and lower bounds for the switching
  regret, i.e., the difference between the loss incurred by the algorithm and the
  optimal strategy in hindsight that switches state at most $L$ times, where $L$ is
  some parameter. We show positive results for online linear optimization and the
  classic experts problem. Interestingly, such results turn out to be impossible for
  the classic bandit setting.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bhaskara23a
month: 0
tex_title: Competing against Adaptive Strategies in Online Learning via Hints
firstpage: 10409
lastpage: 10424
page: 10409-10424
order: 10409
cycles: false
bibtex_author: Bhaskara, Aditya and Munagala, Kamesh
author:
- given: Aditya
  family: Bhaskara
- given: Kamesh
  family: Munagala
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/bhaskara23a/bhaskara23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
