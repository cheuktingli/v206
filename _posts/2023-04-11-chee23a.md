---
title: "“Plus/minus the learning rate”: Easy and Scalable Statistical Inference with
  SGD"
software: https://github.com/jerry-chee/SGDInference
abstract: 'In this paper, we develop a statistical inference procedure using stochastic
  gradient descent (SGD)-based confidence intervals. These intervals are of the simplest
  possible form: $\theta_{N,j} \pm 2\sqrt{}(\gamma/N)$ , where $\theta_N$ is the SGD
  estimate of model parameters $\theta$ over N data points, and $\gamma$ is the learning
  rate. This construction relies only on a proper selection of the learning rate to
  ensure the standard SGD conditions for O(1/n) convergence. The procedure performs
  well in our empirical evaluations, achieving near-nominal coverage intervals scaling
  up to 20$\times$ as many parameters as other SGD-based inference methods. We also
  demonstrate our method’s  practical significance on modeling adverse events in emergency
  general surgery patients using a novel dataset from the Hospital of the University
  of Pennsylvania.'
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chee23a
month: 0
tex_title: "“Plus/minus the learning rate”: Easy and Scalable Statistical Inference
  with SGD"
firstpage: 2285
lastpage: 2309
page: 2285-2309
order: 2285
cycles: false
bibtex_author: Chee, Jerry and Kim, Hwanwoo and Toulis, Panos
author:
- given: Jerry
  family: Chee
- given: Hwanwoo
  family: Kim
- given: Panos
  family: Toulis
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/chee23a/chee23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
