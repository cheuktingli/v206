---
title: Boosted Off-Policy Learning
abstract: We propose the first boosting algorithm for off-policy learning from logged
  bandit feedback. Unlike existing boosting methods for supervised learning, our algorithm
  directly optimizes an estimate of the policy’s expected reward. We analyze this
  algorithm and prove that the excess empirical risk decreases (possibly exponentially
  fast) with each round of boosting, provided a “weak” learning condition is satisfied
  by the base learner. We further show how to reduce the base learner to supervised
  learning, which opens up a broad range of readily available base learners with practical
  benefits, such as decision trees. Experiments indicate that our algorithm inherits
  many desirable properties of tree-based boosting algorithms (e.g., robustness to
  feature scaling and hyperparameter tuning), and that it can outperform off-policy
  learning with deep neural networks as well as methods that simply regress on the
  observed rewards.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: london23a
month: 0
tex_title: Boosted Off-Policy Learning
firstpage: 5614
lastpage: 5640
page: 5614-5640
order: 5614
cycles: false
bibtex_author: London, Ben and Lu, Levi and Sandler, Ted and Joachims, Thorsten
author:
- given: Ben
  family: London
- given: Levi
  family: Lu
- given: Ted
  family: Sandler
- given: Thorsten
  family: Joachims
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/london23a/london23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
