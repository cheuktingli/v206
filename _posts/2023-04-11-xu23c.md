---
title: Linear Convergence of Gradient Descent For Finite Width Over-parametrized Linear
  Networks With General Initialization
abstract: Recent theoretical analyses of the convergence of gradient descent (GD)
  to a global minimum for over-parametrized neural networks make strong assumptions
  on the step size (infinitesimal), the hidden-layer width (infinite), or the initialization
  (spectral, balanced). In this work, we relax these assumptions and derive a linear
  convergence rate for two-layer linear networks trained using GD on the squared loss
  in the case of finite step size, finite width and general initialization. Despite
  the generality of our analysis, our rate estimates are significantly tighter than
  those of prior work. Moreover, we provide a time-varying step size rule that monotonically
  improves the convergence rate as the loss function decreases to zero. Numerical
  experiments validate our findings.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xu23c
month: 0
tex_title: Linear Convergence of Gradient Descent For Finite Width Over-parametrized
  Linear Networks With General Initialization
firstpage: 2262
lastpage: 2284
page: 2262-2284
order: 2262
cycles: false
bibtex_author: Xu, Ziqing and Min, Hancheng and Tarmoun, Salma and Mallada, Enrique
  and Vidal, Rene
author:
- given: Ziqing
  family: Xu
- given: Hancheng
  family: Min
- given: Salma
  family: Tarmoun
- given: Enrique
  family: Mallada
- given: Rene
  family: Vidal
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/xu23c/xu23c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
