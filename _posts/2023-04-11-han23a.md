---
title: Riemannian Accelerated Gradient Methods via Extrapolation
software: https://github.com/andyjm3/RiemNA
abstract: In this paper, we propose a convergence acceleration scheme for general
  Riemannian optimization problems by extrapolating iterates on manifolds. We show
  that when the iterates are generated from the Riemannian gradient descent method,
  the scheme achieves the optimal convergence rate asymptotically and is computationally
  more favorable than the recently proposed Riemannian Nesterov accelerated gradient
  methods. A salient feature of our analysis is the convergence guarantees with respect
  to the use of general retraction and vector transport. Empirically, we verify the
  practical benefits of the proposed acceleration strategy, including robustness to
  the choice of different averaging schemes on manifolds.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: han23a
month: 0
tex_title: Riemannian Accelerated Gradient Methods via Extrapolation
firstpage: 1554
lastpage: 1585
page: 1554-1585
order: 1554
cycles: false
bibtex_author: Han, Andi and Mishra, Bamdev and Jawanpuria, Pratik and Gao, Junbin
author:
- given: Andi
  family: Han
- given: Bamdev
  family: Mishra
- given: Pratik
  family: Jawanpuria
- given: Junbin
  family: Gao
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/han23a/han23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
