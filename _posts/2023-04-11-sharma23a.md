---
title: Do Bayesian Neural Networks Need To Be Fully Stochastic?
software: https://github.com/MrinankSharma/do_bnns_need_to_be_fully_stochastic
abstract: We investigate the benefit of treating all the parameters in a Bayesian
  neural network stochastically and find compelling theoretical and empirical evidence
  that this standard construction may be unnecessary. To this end, we prove that expressive
  predictive distributions require only small amounts of stochasticity. In particular,
  partially stochastic networks with only n stochastic biases are universal probabilistic
  predictors for n-dimensional predictive problems. In empirical investigations, we
  find no systematic benefit of full stochasticity across four different inference
  modalities and eight datasets; partially stochastic networks can match and sometimes
  even outperform fully stochastic networks, despite their reduced memory costs.
section: Notable Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sharma23a
month: 0
tex_title: Do Bayesian Neural Networks Need To Be Fully Stochastic?
firstpage: 7694
lastpage: 7722
page: 7694-7722
order: 7694
cycles: false
bibtex_author: Sharma, Mrinank and Farquhar, Sebastian and Nalisnick, Eric and Rainforth,
  Tom
author:
- given: Mrinank
  family: Sharma
- given: Sebastian
  family: Farquhar
- given: Eric
  family: Nalisnick
- given: Tom
  family: Rainforth
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/sharma23a/sharma23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
