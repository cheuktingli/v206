---
title: Blessing of Class Diversity in Pre-training
abstract: This paper presents a new statistical analysis aiming to explain the recent
  superior achievements of the pre-training techniques in natural language processing
  (NLP). We prove that when the classes of the pre-training task (e.g., different
  words in the masked language model task) are sufficiently diverse, in the sense
  that the least singular value of the last linear layer in pre-training (denoted
  as $\tilde{\nu}$) is large, then pre-training can significantly improve the sample
  efficiency of downstream tasks. Specially, we show the transfer learning excess
  risk enjoys an $O\left(\frac{1}{\tilde{\nu} \sqrt{n}}\right)$ rate, in contrast
  to the $O\left(\frac{1}{\sqrt{m}}\right)$ rate in the standard supervised learning.
  Here, $n$ is the number of pre-training data and $m$ is the number of data in the
  downstream task, and typically $n \gg m$. Our proof relies on a vector-form Rademacher
  complexity chain rule for disassembling composite function classes and a modified
  self-concordance condition. These techniques can be of independent interest.
section: Notable Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhao23a
month: 0
tex_title: Blessing of Class Diversity in Pre-training
firstpage: 283
lastpage: 305
page: 283-305
order: 283
cycles: false
bibtex_author: Zhao, Yulai and Chen, Jianshu and Du, Simon
author:
- given: Yulai
  family: Zhao
- given: Jianshu
  family: Chen
- given: Simon
  family: Du
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/zhao23a/zhao23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
