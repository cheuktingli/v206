---
title: Nyström Method for Accurate and Scalable Implicit Differentiation
software: https://github.com/moskomule/hypergrad
abstract: The essential difficulty of gradient-based bilevel optimization using implicit
  differentiation is to estimate the inverse Hessian vector product with respect to
  neural network parameters. This paper proposes to tackle this problem by the Nyström
  method and the Woodbury matrix identity, exploiting the low-rankness of the Hessian.
  Compared to existing methods using iterative approximation, such as conjugate gradient
  and the Neumann series approximation, the proposed method avoids numerical instability
  and can be efficiently computed in matrix operations without iterations. As a result,
  the proposed method works stably in various tasks and is faster than iterative approximations.
  Throughout experiments including large-scale hyperparameter optimization and meta
  learning, we demonstrate that the Nyström method consistently achieves comparable
  or even superior performance to other approaches. The source code is available from
  https://github.com/moskomule/hypergrad.
section: Regular Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hataya23a
month: 0
tex_title: Nyström Method for Accurate and Scalable Implicit Differentiation
firstpage: 4643
lastpage: 4654
page: 4643-4654
order: 4643
cycles: false
bibtex_author: Hataya, Ryuichiro and Yamada, Makoto
author:
- given: Ryuichiro
  family: Hataya
- given: Makoto
  family: Yamada
date: 2023-04-11
address:
container-title: Proceedings of The 26th International Conference on Artificial Intelligence
  and Statistics
volume: '206'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 4
  - 11
pdf: https://proceedings.mlr.press/v206/hataya23a/hataya23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
